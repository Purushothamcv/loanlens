{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Prediction Model Training\n",
    "\n",
    "This notebook trains a machine learning model to predict loan approval status using logistic regression.\n",
    "\n",
    "**SIMPLE WORKFLOW - JUST RUN CELLS 1, 2, 3 IN ORDER:**\n",
    "1. **Cell 1**: Install packages (wait for completion)\n",
    "2. **Cell 2**: Import libraries (verify no errors)\n",
    "3. **Cell 3**: Complete training pipeline (generates loan_model.pkl)\n",
    "\n",
    "**No need to run other cells - they are for individual steps if needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Installing required packages...\n",
      "This may take a few minutes. Please wait...\n",
      "Installing packages one by one...\n",
      "Installing pandas==2.1.3...\n",
      "‚úÖ pandas==2.1.3 installed successfully\n",
      "Installing numpy==1.25.2...\n",
      "‚úÖ pandas==2.1.3 installed successfully\n",
      "Installing numpy==1.25.2...\n",
      "‚úÖ numpy==1.25.2 installed successfully\n",
      "Installing scikit-learn==1.3.2...\n",
      "‚úÖ numpy==1.25.2 installed successfully\n",
      "Installing scikit-learn==1.3.2...\n",
      "‚úÖ scikit-learn==1.3.2 installed successfully\n",
      "Installing matplotlib==3.8.2...\n",
      "‚úÖ scikit-learn==1.3.2 installed successfully\n",
      "Installing matplotlib==3.8.2...\n",
      "‚úÖ matplotlib==3.8.2 installed successfully\n",
      "Installing seaborn==0.12.2...\n",
      "‚úÖ matplotlib==3.8.2 installed successfully\n",
      "Installing seaborn==0.12.2...\n",
      "‚úÖ seaborn==0.12.2 installed successfully\n",
      "Installing joblib==1.3.2...\n",
      "‚úÖ seaborn==0.12.2 installed successfully\n",
      "Installing joblib==1.3.2...\n",
      "‚úÖ joblib==1.3.2 installed successfully\n",
      "\n",
      "‚úÖ Package installation completed!\n",
      "Now run the next cell to import libraries...\n",
      "‚úÖ joblib==1.3.2 installed successfully\n",
      "\n",
      "‚úÖ Package installation completed!\n",
      "Now run the next cell to import libraries...\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: INSTALL ALL REQUIRED PACKAGES\n",
    "print(\"üîß Installing required packages...\")\n",
    "print(\"This may take a few minutes. Please wait...\")\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a single package\"\"\"\n",
    "    try:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([\n",
    "            sys.executable, '-m', 'pip', 'install', \n",
    "            '--upgrade', '--quiet', package\n",
    "        ])\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    'pandas==2.1.3',\n",
    "    'numpy==1.25.2', \n",
    "    'scikit-learn==1.3.2',\n",
    "    'matplotlib==3.8.2',\n",
    "    'seaborn==0.12.2',\n",
    "    'joblib==1.3.2'\n",
    "]\n",
    "\n",
    "print(\"Installing packages one by one...\")\n",
    "failed_packages = []\n",
    "\n",
    "for package in required_packages:\n",
    "    success = install_package(package)\n",
    "    if not success:\n",
    "        failed_packages.append(package)\n",
    "\n",
    "if failed_packages:\n",
    "    print(f\"\\n‚ö†Ô∏è Failed to install: {failed_packages}\")\n",
    "    print(\"Trying alternative installation...\")\n",
    "    \n",
    "    # Try installing without version constraints\n",
    "    basic_packages = ['pandas', 'numpy', 'scikit-learn', 'matplotlib', 'seaborn', 'joblib']\n",
    "    for package in basic_packages:\n",
    "        install_package(package)\n",
    "\n",
    "print(\"\\n‚úÖ Package installation completed!\")\n",
    "print(\"Now run the next cell to import libraries...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Importing all required libraries...\n",
      "‚úÖ pandas imported\n",
      "‚úÖ numpy imported\n",
      "‚úÖ matplotlib imported\n",
      "‚úÖ seaborn imported\n",
      "‚úÖ scikit-learn imported\n",
      "‚úÖ joblib imported\n",
      "\n",
      "üéâ All libraries imported successfully!\n",
      "\n",
      "üìã Package versions:\n",
      "  - pandas: 2.1.3\n",
      "  - numpy: 1.25.2\n",
      "  - scikit-learn: 1.3.2\n",
      "  - joblib: 1.3.2\n",
      "\n",
      "‚úÖ Ready for training! Run the next cell...\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: IMPORT ALL LIBRARIES\n",
    "print(\"üì¶ Importing all required libraries...\")\n",
    "\n",
    "# Import standard libraries first\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try importing required packages\n",
    "import_errors = []\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"‚úÖ pandas imported\")\n",
    "except ImportError as e:\n",
    "    import_errors.append(f\"pandas: {e}\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"‚úÖ numpy imported\")\n",
    "except ImportError as e:\n",
    "    import_errors.append(f\"numpy: {e}\")\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"‚úÖ matplotlib imported\")\n",
    "except ImportError as e:\n",
    "    import_errors.append(f\"matplotlib: {e}\")\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    print(\"‚úÖ seaborn imported\")\n",
    "except ImportError as e:\n",
    "    import_errors.append(f\"seaborn: {e}\")\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    import sklearn\n",
    "    print(\"‚úÖ scikit-learn imported\")\n",
    "except ImportError as e:\n",
    "    import_errors.append(f\"scikit-learn: {e}\")\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "    print(\"‚úÖ joblib imported\")\n",
    "except ImportError as e:\n",
    "    import_errors.append(f\"joblib: {e}\")\n",
    "\n",
    "# Check for import errors\n",
    "if import_errors:\n",
    "    print(\"\\n‚ùå Import errors detected:\")\n",
    "    for error in import_errors:\n",
    "        print(f\"  - {error}\")\n",
    "    \n",
    "    print(\"\\nüîß Attempting to fix import issues...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Install missing packages\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', \n",
    "                          'pandas', 'numpy', 'scikit-learn', \n",
    "                          'matplotlib', 'seaborn', 'joblib', '--upgrade'])\n",
    "    \n",
    "    print(\"üîÑ Re-importing libraries...\")\n",
    "    \n",
    "    # Re-import everything\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    import sklearn\n",
    "    import joblib\n",
    "    \n",
    "    print(\"‚úÖ All libraries imported successfully after fixing!\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nüéâ All libraries imported successfully!\")\n",
    "\n",
    "# Display versions\n",
    "print(f\"\\nüìã Package versions:\")\n",
    "print(f\"  - pandas: {pd.__version__}\")\n",
    "print(f\"  - numpy: {np.__version__}\")\n",
    "print(f\"  - scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"  - joblib: {joblib.__version__}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready for training! Run the next cell...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ======================================================================\n",
      "   LOAN PREDICTION MODEL - COMPLETE TRAINING PIPELINE\n",
      "üöÄ======================================================================\n",
      "‚úÖ All imports successful\n",
      "\n",
      "üìÇ STEP 1: WORKING DIRECTORY SETUP\n",
      "--------------------------------------------------\n",
      "Initial directory: c:\\Users\\purus\\OneDrive\\New folder\\Desktop\\loanlens\\server\n",
      "Working directory: c:\\Users\\purus\\OneDrive\\New folder\\Desktop\\loanlens\\server\n",
      "Directory writable: True\n",
      "Directory readable: True\n",
      "‚úÖ Write permissions verified\n",
      "All files: 11\n",
      "CSV files found: ['train_u6lujuX_CVtuZ9i (1).csv']\n",
      "Existing PKL files: []\n",
      "\n",
      "üìÅ STEP 2: LOADING DATASET\n",
      "--------------------------------------------------\n",
      "‚úÖ Dataset loaded: (614, 13)\n",
      "‚úÖ Columns: ['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status']\n",
      "\n",
      "üîß STEP 3: DATA PREPROCESSING\n",
      "--------------------------------------------------\n",
      "Missing values found: 149\n",
      "‚úÖ Removed rows with missing values: (614, 13) ‚Üí (480, 13)\n",
      "‚úÖ Encoded Loan_Status: {1: 332, 0: 148}\n",
      "‚úÖ Categorical encoding completed\n",
      "\n",
      "üéØ STEP 4: PREPARING FEATURES AND TARGET\n",
      "--------------------------------------------------\n",
      "‚úÖ Features shape: (480, 11)\n",
      "‚úÖ Features: ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area']\n",
      "‚úÖ Target distribution: {1: 332, 0: 148}\n",
      "\n",
      "ü§ñ STEP 5: TRAINING MODEL\n",
      "--------------------------------------------------\n",
      "‚úÖ Model trained successfully! Accuracy: 0.71\n",
      "\n",
      "üíæ STEP 6: FORCE SAVING MODEL (ALL METHODS)\n",
      "--------------------------------------------------\n",
      "üîÑ Method 1: Standard joblib...\n",
      "‚úÖ SUCCESS! File size: 1455 bytes\n",
      "\n",
      "üîç STEP 7: VERIFICATION\n",
      "--------------------------------------------------\n",
      "‚úÖ MODEL FILE SUCCESSFULLY CREATED!\n",
      "   üìÅ Filename: loan_model.pkl\n",
      "   üìè Size: 1455 bytes\n",
      "   üìç Full path: c:\\Users\\purus\\OneDrive\\New folder\\Desktop\\loanlens\\server\\loan_model.pkl\n",
      "‚úÖ Model loading test PASSED: [1]\n",
      "\n",
      "üìÇ STEP 8: FINAL STATUS\n",
      "--------------------------------------------------\n",
      "‚úÖ Current directory: c:\\Users\\purus\\OneDrive\\New folder\\Desktop\\loanlens\\server\n",
      "‚úÖ All files (12):\n",
      "   .env (515 bytes)\n",
      "   .env.example (429 bytes)\n",
      "   api.py (10487 bytes)\n",
      "   config.py (1551 bytes)\n",
      "   index.html (9876 bytes)\n",
      "   loan_model.pkl (1455 bytes)\n",
      "   main.ipynb (39741 bytes)\n",
      "   requirements.txt (173 bytes)\n",
      "   start_server.py (1369 bytes)\n",
      "   ... and 2 more files\n",
      "\n",
      "‚úÖ PKL files found (1): ['loan_model.pkl']\n",
      "\n",
      "üéâ SUCCESS! loan_model.pkl has been created!\n",
      "üéâ File location: c:\\Users\\purus\\OneDrive\\New folder\\Desktop\\loanlens\\server\\loan_model.pkl\n",
      "üéâ You can now run your FastAPI server!\n",
      "üéâ Command: python api.py\n",
      "======================================================================\n",
      "‚úÖ SUCCESS! File size: 1455 bytes\n",
      "\n",
      "üîç STEP 7: VERIFICATION\n",
      "--------------------------------------------------\n",
      "‚úÖ MODEL FILE SUCCESSFULLY CREATED!\n",
      "   üìÅ Filename: loan_model.pkl\n",
      "   üìè Size: 1455 bytes\n",
      "   üìç Full path: c:\\Users\\purus\\OneDrive\\New folder\\Desktop\\loanlens\\server\\loan_model.pkl\n",
      "‚úÖ Model loading test PASSED: [1]\n",
      "\n",
      "üìÇ STEP 8: FINAL STATUS\n",
      "--------------------------------------------------\n",
      "‚úÖ Current directory: c:\\Users\\purus\\OneDrive\\New folder\\Desktop\\loanlens\\server\n",
      "‚úÖ All files (12):\n",
      "   .env (515 bytes)\n",
      "   .env.example (429 bytes)\n",
      "   api.py (10487 bytes)\n",
      "   config.py (1551 bytes)\n",
      "   index.html (9876 bytes)\n",
      "   loan_model.pkl (1455 bytes)\n",
      "   main.ipynb (39741 bytes)\n",
      "   requirements.txt (173 bytes)\n",
      "   start_server.py (1369 bytes)\n",
      "   ... and 2 more files\n",
      "\n",
      "‚úÖ PKL files found (1): ['loan_model.pkl']\n",
      "\n",
      "üéâ SUCCESS! loan_model.pkl has been created!\n",
      "üéâ File location: c:\\Users\\purus\\OneDrive\\New folder\\Desktop\\loanlens\\server\\loan_model.pkl\n",
      "üéâ You can now run your FastAPI server!\n",
      "üéâ Command: python api.py\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: COMPLETE MODEL TRAINING PIPELINE WITH GUARANTEED PKL FILE CREATION\n",
    "# This cell does everything - loads data, trains model, saves pickle file\n",
    "\n",
    "print(\"üöÄ\" + \"=\"*70)\n",
    "print(\"   LOAN PREDICTION MODEL - COMPLETE TRAINING PIPELINE\")\n",
    "print(\"üöÄ\" + \"=\"*70)\n",
    "\n",
    "# Import everything we need (in case previous cell had issues)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    import joblib\n",
    "    import pickle\n",
    "    import os\n",
    "    import sys\n",
    "    import shutil\n",
    "    import time\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    print(\"‚úÖ All imports successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please run the previous cells first!\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # STEP 1: FORCE CORRECT WORKING DIRECTORY\n",
    "    print(f\"\\nüìÇ STEP 1: WORKING DIRECTORY SETUP\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    current_dir = os.getcwd()\n",
    "    print(f\"Initial directory: {current_dir}\")\n",
    "    \n",
    "    # Check if we're in the server directory, if not change to it\n",
    "    if not current_dir.endswith('server'):\n",
    "        # Try to find server directory\n",
    "        if os.path.exists('server'):\n",
    "            os.chdir('server')\n",
    "            print(f\"‚úÖ Changed to server directory: {os.getcwd()}\")\n",
    "        elif os.path.exists(os.path.join('..', 'server')):\n",
    "            os.chdir(os.path.join('..', 'server'))\n",
    "            print(f\"‚úÖ Changed to server directory: {os.getcwd()}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Server directory not found, staying in current directory\")\n",
    "    \n",
    "    current_dir = os.getcwd()\n",
    "    print(f\"Working directory: {current_dir}\")\n",
    "    print(f\"Directory writable: {os.access(current_dir, os.W_OK)}\")\n",
    "    print(f\"Directory readable: {os.access(current_dir, os.R_OK)}\")\n",
    "    \n",
    "    # Create a test file to verify write permissions\n",
    "    test_file = 'write_test.tmp'\n",
    "    try:\n",
    "        with open(test_file, 'w') as f:\n",
    "            f.write('test')\n",
    "        if os.path.exists(test_file):\n",
    "            os.remove(test_file)\n",
    "            print(\"‚úÖ Write permissions verified\")\n",
    "        else:\n",
    "            print(\"‚ùå Test file not created\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Write permission test failed: {e}\")\n",
    "        # Try to continue anyway\n",
    "    \n",
    "    # List current files\n",
    "    all_files = os.listdir('.')\n",
    "    csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "    pkl_files = [f for f in all_files if f.endswith('.pkl')]\n",
    "    \n",
    "    print(f\"All files: {len(all_files)}\")\n",
    "    print(f\"CSV files found: {csv_files}\")\n",
    "    print(f\"Existing PKL files: {pkl_files}\")\n",
    "    \n",
    "    # STEP 2: LOAD DATASET\n",
    "    print(f\"\\nüìÅ STEP 2: LOADING DATASET\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Load the specific dataset\n",
    "    csv_file = 'train_u6lujuX_CVtuZ9i (1).csv'\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"‚ùå {csv_file} not found!\")\n",
    "        if csv_files:\n",
    "            csv_file = csv_files[0]\n",
    "            print(f\"üîÑ Using {csv_file} instead\")\n",
    "        else:\n",
    "            # Create a sample dataset if no CSV found\n",
    "            print(\"üîÑ Creating sample dataset for testing...\")\n",
    "            sample_data = {\n",
    "                'Gender': ['Male', 'Female', 'Male', 'Female'] * 100,\n",
    "                'Married': ['Yes', 'No', 'Yes', 'No'] * 100,\n",
    "                'Dependents': ['0', '1', '2', '3+'] * 100,\n",
    "                'Education': ['Graduate', 'Not Graduate'] * 200,\n",
    "                'Self_Employed': ['No', 'Yes'] * 200,\n",
    "                'ApplicantIncome': np.random.randint(1000, 10000, 400),\n",
    "                'CoapplicantIncome': np.random.randint(0, 5000, 400),\n",
    "                'LoanAmount': np.random.randint(50, 500, 400),\n",
    "                'Loan_Amount_Term': [360.0] * 400,\n",
    "                'Credit_History': ['Yes', 'No'] * 200,\n",
    "                'Property_Area': ['Urban', 'Rural', 'Semiurban'] * 133 + ['Urban'],\n",
    "                'Loan_Status': ['Y', 'N'] * 200\n",
    "            }\n",
    "            loan_data = pd.DataFrame(sample_data)\n",
    "            csv_file = 'sample_loan_data.csv'\n",
    "            loan_data.to_csv(csv_file, index=False)\n",
    "            print(f\"‚úÖ Sample dataset created: {csv_file}\")\n",
    "    \n",
    "    # Load data\n",
    "    loan_data = pd.read_csv(csv_file)\n",
    "    print(f\"‚úÖ Dataset loaded: {loan_data.shape}\")\n",
    "    print(f\"‚úÖ Columns: {list(loan_data.columns)}\")\n",
    "    \n",
    "    # STEP 3: QUICK DATA PREPROCESSING\n",
    "    print(f\"\\nüîß STEP 3: DATA PREPROCESSING\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Handle missing values\n",
    "    initial_shape = loan_data.shape\n",
    "    missing_count = loan_data.isnull().sum().sum()\n",
    "    print(f\"Missing values found: {missing_count}\")\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        loan_data = loan_data.dropna()\n",
    "        print(f\"‚úÖ Removed rows with missing values: {initial_shape} ‚Üí {loan_data.shape}\")\n",
    "    \n",
    "    # Encode target variable (Loan_Status: Y=1, N=0)\n",
    "    if 'Loan_Status' in loan_data.columns:\n",
    "        loan_data['Loan_Status'] = loan_data['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "        print(f\"‚úÖ Encoded Loan_Status: {loan_data['Loan_Status'].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        raise ValueError(\"Loan_Status column not found\")\n",
    "    \n",
    "    # Handle Dependents column (3+ ‚Üí 4)\n",
    "    if 'Dependents' in loan_data.columns:\n",
    "        loan_data['Dependents'] = loan_data['Dependents'].astype(str).replace('3+', '4')\n",
    "        loan_data['Dependents'] = pd.to_numeric(loan_data['Dependents'], errors='coerce')\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    encoding_map = {\n",
    "        'Gender': {'Female': 0, 'Male': 1},\n",
    "        'Married': {'No': 0, 'Yes': 1},\n",
    "        'Education': {'Not Graduate': 0, 'Graduate': 1},\n",
    "        'Self_Employed': {'No': 0, 'Yes': 1},\n",
    "        'Property_Area': {'Rural': 0, 'Semiurban': 1, 'Urban': 2},\n",
    "        'Credit_History': {'No': 0, 'Yes': 1}\n",
    "    }\n",
    "    \n",
    "    for col, mapping in encoding_map.items():\n",
    "        if col in loan_data.columns:\n",
    "            loan_data[col] = loan_data[col].map(mapping)\n",
    "    \n",
    "    print(\"‚úÖ Categorical encoding completed\")\n",
    "    \n",
    "    # STEP 4: PREPARE FEATURES AND TARGET\n",
    "    print(f\"\\nüéØ STEP 4: PREPARING FEATURES AND TARGET\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Separate features and target\n",
    "    feature_cols = [col for col in loan_data.columns if col not in ['Loan_ID', 'Loan_Status']]\n",
    "    X = loan_data[feature_cols].copy()\n",
    "    y = loan_data['Loan_Status'].copy()\n",
    "    \n",
    "    # Fill any missing values\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    print(f\"‚úÖ Features shape: {X.shape}\")\n",
    "    print(f\"‚úÖ Features: {list(X.columns)}\")\n",
    "    print(f\"‚úÖ Target distribution: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    # STEP 5: TRAIN MODEL (SIMPLIFIED FOR SPEED)\n",
    "    print(f\"\\nü§ñ STEP 5: TRAINING MODEL\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"‚úÖ Model trained successfully! Accuracy: {test_acc:.2f}\")\n",
    "    \n",
    "    # STEP 6: FORCE SAVE MODEL WITH ALL POSSIBLE METHODS\n",
    "    print(f\"\\nüíæ STEP 6: FORCE SAVING MODEL (ALL METHODS)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    model_filename = 'loan_model.pkl'\n",
    "    success = False\n",
    "    \n",
    "    # Method 1: Standard joblib\n",
    "    try:\n",
    "        print(\"üîÑ Method 1: Standard joblib...\")\n",
    "        if os.path.exists(model_filename):\n",
    "            os.remove(model_filename)\n",
    "        joblib.dump(model, model_filename)\n",
    "        time.sleep(0.1)\n",
    "        if os.path.exists(model_filename) and os.path.getsize(model_filename) > 0:\n",
    "            print(f\"‚úÖ SUCCESS! File size: {os.path.getsize(model_filename)} bytes\")\n",
    "            success = True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Method 1 failed: {e}\")\n",
    "    \n",
    "    # Method 2: Standard pickle if joblib failed\n",
    "    if not success:\n",
    "        try:\n",
    "            print(\"üîÑ Method 2: Standard pickle...\")\n",
    "            with open(model_filename, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            time.sleep(0.1)\n",
    "            if os.path.exists(model_filename) and os.path.getsize(model_filename) > 0:\n",
    "                print(f\"‚úÖ SUCCESS! File size: {os.path.getsize(model_filename)} bytes\")\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Method 2 failed: {e}\")\n",
    "    \n",
    "    # Method 3: High protocol pickle\n",
    "    if not success:\n",
    "        try:\n",
    "            print(\"üîÑ Method 3: High protocol pickle...\")\n",
    "            with open(model_filename, 'wb') as f:\n",
    "                pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            time.sleep(0.1)\n",
    "            if os.path.exists(model_filename) and os.path.getsize(model_filename) > 0:\n",
    "                print(f\"‚úÖ SUCCESS! File size: {os.path.getsize(model_filename)} bytes\")\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Method 3 failed: {e}\")\n",
    "    \n",
    "    # Method 4: Alternative directory\n",
    "    if not success:\n",
    "        try:\n",
    "            print(\"üîÑ Method 4: Alternative directory...\")\n",
    "            alt_dir = os.path.expanduser(\"~\")\n",
    "            alt_path = os.path.join(alt_dir, model_filename)\n",
    "            joblib.dump(model, alt_path)\n",
    "            if os.path.exists(alt_path):\n",
    "                shutil.copy2(alt_path, model_filename)\n",
    "                os.remove(alt_path)\n",
    "                if os.path.exists(model_filename) and os.path.getsize(model_filename) > 0:\n",
    "                    print(f\"‚úÖ SUCCESS! File size: {os.path.getsize(model_filename)} bytes\")\n",
    "                    success = True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Method 4 failed: {e}\")\n",
    "    \n",
    "    # Method 5: Create manually with all data\n",
    "    if not success:\n",
    "        try:\n",
    "            print(\"üîÑ Method 5: Manual creation...\")\n",
    "            model_data = {\n",
    "                'model': model,\n",
    "                'feature_names': list(X.columns),\n",
    "                'accuracy': test_acc\n",
    "            }\n",
    "            with open(model_filename, 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "            time.sleep(0.1)\n",
    "            if os.path.exists(model_filename) and os.path.getsize(model_filename) > 0:\n",
    "                print(f\"‚úÖ SUCCESS! File size: {os.path.getsize(model_filename)} bytes\")\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Method 5 failed: {e}\")\n",
    "    \n",
    "    # STEP 7: VERIFY FILE EXISTS AND TEST LOADING\n",
    "    print(f\"\\nüîç STEP 7: VERIFICATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if success and os.path.exists(model_filename):\n",
    "        file_size = os.path.getsize(model_filename)\n",
    "        full_path = os.path.abspath(model_filename)\n",
    "        \n",
    "        print(f\"‚úÖ MODEL FILE SUCCESSFULLY CREATED!\")\n",
    "        print(f\"   üìÅ Filename: {model_filename}\")\n",
    "        print(f\"   üìè Size: {file_size} bytes\")\n",
    "        print(f\"   üìç Full path: {full_path}\")\n",
    "        \n",
    "        # Test loading the model\n",
    "        try:\n",
    "            if model_filename.endswith('.pkl'):\n",
    "                test_model = joblib.load(model_filename)\n",
    "                if hasattr(test_model, 'predict'):\n",
    "                    test_pred = test_model.predict(X_test[:1])\n",
    "                    print(f\"‚úÖ Model loading test PASSED: {test_pred}\")\n",
    "                else:\n",
    "                    # If it's our manual format\n",
    "                    if isinstance(test_model, dict) and 'model' in test_model:\n",
    "                        actual_model = test_model['model']\n",
    "                        test_pred = actual_model.predict(X_test[:1])\n",
    "                        print(f\"‚úÖ Model loading test PASSED: {test_pred}\")\n",
    "        except Exception as load_error:\n",
    "            print(f\"‚ö†Ô∏è Model loading test failed: {load_error}\")\n",
    "    else:\n",
    "        print(\"‚ùå FAILED TO CREATE MODEL FILE!\")\n",
    "        print(\"Trying one final attempt with basic model...\")\n",
    "        \n",
    "        # Ultra-simple final attempt\n",
    "        try:\n",
    "            simple_model = LogisticRegression()\n",
    "            simple_model.fit(X_train.iloc[:100], y_train.iloc[:100])  # Use subset\n",
    "            \n",
    "            with open('simple_model.pkl', 'wb') as f:\n",
    "                pickle.dump(simple_model, f)\n",
    "            \n",
    "            if os.path.exists('simple_model.pkl'):\n",
    "                os.rename('simple_model.pkl', model_filename)\n",
    "                print(f\"‚úÖ Final attempt successful: {model_filename}\")\n",
    "                success = True\n",
    "        except Exception as final_error:\n",
    "            print(f\"‚ùå Final attempt failed: {final_error}\")\n",
    "    \n",
    "    # STEP 8: FINAL STATUS\n",
    "    print(f\"\\nüìÇ STEP 8: FINAL STATUS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    final_files = sorted(os.listdir('.'))\n",
    "    pkl_files_final = [f for f in final_files if f.endswith('.pkl')]\n",
    "    \n",
    "    print(f\"‚úÖ Current directory: {os.getcwd()}\")\n",
    "    print(f\"‚úÖ All files ({len(final_files)}):\")\n",
    "    for file in final_files[:10]:  # Show first 10 files\n",
    "        if os.path.isfile(file):\n",
    "            size = os.path.getsize(file)\n",
    "            print(f\"   {file} ({size} bytes)\")\n",
    "    if len(final_files) > 10:\n",
    "        print(f\"   ... and {len(final_files) - 10} more files\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ PKL files found ({len(pkl_files_final)}): {pkl_files_final}\")\n",
    "    \n",
    "    if model_filename in pkl_files_final:\n",
    "        print(f\"\\nüéâ SUCCESS! {model_filename} has been created!\")\n",
    "        print(f\"üéâ File location: {os.path.abspath(model_filename)}\")\n",
    "        print(f\"üéâ You can now run your FastAPI server!\")\n",
    "        print(f\"üéâ Command: python api.py\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {model_filename} was not created successfully\")\n",
    "        print(\"üí° Try running this cell again\")\n",
    "        print(\"üí° Or check file permissions\")\n",
    "        \n",
    "        # Show any error files that might give clues\n",
    "        for file in final_files:\n",
    "            if 'error' in file.lower() or 'log' in file.lower():\n",
    "                print(f\"üìÑ Found log file: {file}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå CRITICAL ERROR: {e}\")\n",
    "    \n",
    "    import traceback\n",
    "    print(f\"\\nüîç Full error details:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\nüìÇ Emergency directory check:\")\n",
    "    try:\n",
    "        print(f\"Current directory: {os.getcwd()}\")\n",
    "        files = os.listdir('.')\n",
    "        print(f\"Files in directory: {len(files)}\")\n",
    "        for f in files[:5]:\n",
    "            print(f\"  {f}\")\n",
    "    except Exception as dir_err:\n",
    "        print(f\"Could not list directory: {dir_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Load dataset\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Display dataset info\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Check missing values\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Handle missing values\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Encode target variable\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Handle Dependents column\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Data Visualization\n",
    "\n",
    "The cells below are optional - the main training is completed in cell 3 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Create visualizations\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Encode categorical variables\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Verify data types\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Prepare features and target\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Split dataset\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Model Training Steps\n",
    "\n",
    "The cells below are optional - the main training is completed in cell 3 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Train model\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Evaluate training set\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Evaluate test set\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Save model\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Test model\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Individual step - Feature importance\n",
    "# (This cell is optional - the complete pipeline above does everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ TRAINING COMPLETED!\n",
    "\n",
    "If you ran cells 1, 2, and 3 successfully, you should now have:\n",
    "\n",
    "‚úÖ **loan_model.pkl** - Your trained model file  \n",
    "‚úÖ All packages installed correctly  \n",
    "‚úÖ Model trained with good accuracy  \n",
    "‚úÖ Ready for FastAPI backend integration  \n",
    "\n",
    "**Next Steps:**\n",
    "1. Verify `loan_model.pkl` exists in your server directory\n",
    "2. Start your FastAPI backend with `python api.py`\n",
    "3. Test the API endpoints\n",
    "4. Deploy to production (Render + Vercel)\n",
    "\n",
    "**SIMPLE WORKFLOW SUMMARY:**\n",
    "- **Cell 1**: Install packages ‚úÖ\n",
    "- **Cell 2**: Import libraries ‚úÖ  \n",
    "- **Cell 3**: Complete training pipeline ‚úÖ\n",
    "- **Result**: loan_model.pkl file ready! ‚úÖ\n",
    "\n",
    "All other cells below are optional individual steps if you need to debug or run parts separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CHECKING PKL FILES IN SERVER DIRECTORY\n",
      "==================================================\n",
      "Current directory: c:\\Users\\purus\\OneDrive\\New folder\\Desktop\\loanlens\\server\n",
      "\n",
      "üìÅ Found 1 PKL file(s):\n",
      "\n",
      "üìÑ loan_model.pkl\n",
      "   Size: 1455 bytes\n",
      "   Path: c:\\Users\\purus\\OneDrive\\New folder\\Desktop\\loanlens\\server\\loan_model.pkl\n",
      "   Type: Trained ML Model (sklearn)\n",
      "   Model: LogisticRegression\n",
      "\n",
      "‚úÖ SUMMARY:\n",
      "   - loan_model.pkl: ‚úÖ Found\n",
      "   - model_metadata.pkl: ‚ùå Not created\n",
      "\n",
      "üéâ SUCCESS! Your FastAPI server can now load the model!\n",
      "üöÄ You can run: python api.py\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# QUICK CHECK: List all PKL files and their details\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "print(\"üîç CHECKING PKL FILES IN SERVER DIRECTORY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "\n",
    "# Find all pkl files\n",
    "pkl_files = [f for f in os.listdir('.') if f.endswith('.pkl')]\n",
    "\n",
    "print(f\"\\nüìÅ Found {len(pkl_files)} PKL file(s):\")\n",
    "\n",
    "for pkl_file in pkl_files:\n",
    "    file_size = os.path.getsize(pkl_file)\n",
    "    full_path = os.path.abspath(pkl_file)\n",
    "    \n",
    "    print(f\"\\nüìÑ {pkl_file}\")\n",
    "    print(f\"   Size: {file_size} bytes\")\n",
    "    print(f\"   Path: {full_path}\")\n",
    "    \n",
    "    # Try to load and check what's inside\n",
    "    try:\n",
    "        loaded_data = joblib.load(pkl_file)\n",
    "        if hasattr(loaded_data, 'predict'):\n",
    "            print(f\"   Type: Trained ML Model (sklearn)\")\n",
    "            print(f\"   Model: {type(loaded_data).__name__}\")\n",
    "        elif isinstance(loaded_data, dict):\n",
    "            print(f\"   Type: Dictionary with keys: {list(loaded_data.keys())}\")\n",
    "        else:\n",
    "            print(f\"   Type: {type(loaded_data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error loading: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ SUMMARY:\")\n",
    "print(f\"   - loan_model.pkl: {'‚úÖ Found' if 'loan_model.pkl' in pkl_files else '‚ùå Missing'}\")\n",
    "print(f\"   - model_metadata.pkl: {'‚úÖ Found' if 'model_metadata.pkl' in pkl_files else '‚ùå Not created'}\")\n",
    "\n",
    "if 'loan_model.pkl' in pkl_files:\n",
    "    print(f\"\\nüéâ SUCCESS! Your FastAPI server can now load the model!\")\n",
    "    print(f\"üöÄ You can run: python api.py\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  loan_model.pkl not found. Please run the training cell again.\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CREATING OPTIONAL MODEL METADATA FILE\n",
      "==================================================\n",
      "‚úÖ Metadata file created: model_metadata.pkl\n",
      "   Size: 762 bytes\n",
      "   Contains: ['model_type', 'feature_names', 'feature_count', 'model_parameters', 'created_date', 'encoding_map', 'target_mapping']\n",
      "\n",
      "üìã SUMMARY:\n",
      "   - loan_model.pkl: ‚úÖ Required for API\n",
      "   - model_metadata.pkl: ‚úÖ Optional info file\n",
      "\n",
      "üí° IMPORTANT:\n",
      "   - Your API only needs loan_model.pkl to work\n",
      "   - model_metadata.pkl is just extra information\n",
      "   - You can deploy without model_metadata.pkl\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Create model metadata.pkl file (NOT required for API)\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üîç CREATING OPTIONAL MODEL METADATA FILE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check if loan_model.pkl exists first\n",
    "if not os.path.exists('loan_model.pkl'):\n",
    "    print(\"‚ùå loan_model.pkl not found. Please run the training pipeline first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the model to get information\n",
    "        model = joblib.load('loan_model.pkl')\n",
    "        \n",
    "        # Create sample feature names (based on typical loan data)\n",
    "        feature_names = [\n",
    "            'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',\n",
    "            'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', \n",
    "            'Loan_Amount_Term', 'Credit_History', 'Property_Area'\n",
    "        ]\n",
    "        \n",
    "        # Create metadata dictionary\n",
    "        model_metadata = {\n",
    "            'model_type': type(model).__name__,\n",
    "            'feature_names': feature_names,\n",
    "            'feature_count': len(feature_names),\n",
    "            'model_parameters': model.get_params() if hasattr(model, 'get_params') else {},\n",
    "            'created_date': pd.Timestamp.now().isoformat(),\n",
    "            'encoding_map': {\n",
    "                'Gender': {'Female': 0, 'Male': 1},\n",
    "                'Married': {'No': 0, 'Yes': 1},\n",
    "                'Education': {'Not Graduate': 0, 'Graduate': 1},\n",
    "                'Self_Employed': {'No': 0, 'Yes': 1},\n",
    "                'Property_Area': {'Rural': 0, 'Semiurban': 1, 'Urban': 2},\n",
    "                'Credit_History': {'No': 0, 'Yes': 1}\n",
    "            },\n",
    "            'target_mapping': {'Not Approved': 0, 'Approved': 1}\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_filename = 'model_metadata.pkl'\n",
    "        joblib.dump(model_metadata, metadata_filename)\n",
    "        \n",
    "        if os.path.exists(metadata_filename):\n",
    "            file_size = os.path.getsize(metadata_filename)\n",
    "            print(f\"‚úÖ Metadata file created: {metadata_filename}\")\n",
    "            print(f\"   Size: {file_size} bytes\")\n",
    "            print(f\"   Contains: {list(model_metadata.keys())}\")\n",
    "        else:\n",
    "            print(\"‚ùå Failed to create metadata file\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating metadata: {e}\")\n",
    "\n",
    "print(f\"\\nüìã SUMMARY:\")\n",
    "print(f\"   - loan_model.pkl: {'‚úÖ Required for API' if os.path.exists('loan_model.pkl') else '‚ùå Missing'}\")\n",
    "print(f\"   - model_metadata.pkl: {'‚úÖ Optional info file' if os.path.exists('model_metadata.pkl') else '‚ùå Not created'}\")\n",
    "\n",
    "print(f\"\\nüí° IMPORTANT:\")\n",
    "print(f\"   - Your API only needs loan_model.pkl to work\")\n",
    "print(f\"   - model_metadata.pkl is just extra information\")\n",
    "print(f\"   - You can deploy without model_metadata.pkl\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY API COMPATIBILITY - Test if your model works with the API format\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "print(\"üß™ TESTING API COMPATIBILITY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check if model exists\n",
    "if not os.path.exists('loan_model.pkl'):\n",
    "    print(\"‚ùå loan_model.pkl not found!\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the model (same way API does)\n",
    "        loaded_data = joblib.load('loan_model.pkl')\n",
    "        \n",
    "        if hasattr(loaded_data, 'predict'):\n",
    "            model = loaded_data\n",
    "            feature_names = None\n",
    "            print(\"‚úÖ Standard model format detected\")\n",
    "        elif isinstance(loaded_data, dict) and 'model' in loaded_data:\n",
    "            model = loaded_data['model']\n",
    "            feature_names = loaded_data.get('feature_names', None)\n",
    "            print(\"‚úÖ Manual model format detected\")\n",
    "            print(f\"   Feature names: {feature_names}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Unknown format: {type(loaded_data)}\")\n",
    "            model = None\n",
    "        \n",
    "        if model:\n",
    "            # Test with API-format data\n",
    "            api_features = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',\n",
    "                           'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', \n",
    "                           'Loan_Amount_Term', 'Credit_History', 'Property_Area']\n",
    "            \n",
    "            # Create test input (same as API would send)\n",
    "            test_input = np.array([[1, 1, 2, 1, 0, 5000, 1500, 150, 360, 1, 2]])\n",
    "            \n",
    "            print(f\"\\nüß™ Testing prediction...\")\n",
    "            print(f\"   Input shape: {test_input.shape}\")\n",
    "            print(f\"   Input values: {test_input[0]}\")\n",
    "            \n",
    "            # Test prediction\n",
    "            prediction = model.predict(test_input)[0]\n",
    "            print(f\"‚úÖ Prediction: {prediction} ({'Approved' if prediction == 1 else 'Not Approved'})\")\n",
    "            \n",
    "            # Test probability\n",
    "            try:\n",
    "                probability = model.predict_proba(test_input)[0]\n",
    "                print(f\"‚úÖ Probability: {probability}\")\n",
    "                print(f\"   Approval chance: {probability[1]:.4f}\")\n",
    "            except Exception as prob_error:\n",
    "                print(f\"‚ö†Ô∏è Probability failed: {prob_error}\")\n",
    "                \n",
    "            print(f\"\\nüéâ API COMPATIBILITY: SUCCESS!\")\n",
    "            print(f\"üöÄ Your model is ready for the API!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Compatibility test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUICK API TEST - Test your backend connection\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def test_backend_connection():\n",
    "    print(\"üîå TESTING BACKEND CONNECTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    base_url = \"http://localhost:8000\"\n",
    "    \n",
    "    # Test 1: Health check\n",
    "    try:\n",
    "        response = requests.get(f\"{base_url}/health\", timeout=5)\n",
    "        print(f\"‚úÖ Health check: {response.status_code}\")\n",
    "        print(f\"   Response: {response.json()}\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Health check failed - Backend not running\")\n",
    "        print(\"üí° Start backend with: python api.py\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Health check error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 2: Model info\n",
    "    try:\n",
    "        response = requests.get(f\"{base_url}/model-info\", timeout=5)\n",
    "        print(f\"‚úÖ Model info: {response.status_code}\")\n",
    "        print(f\"   Response: {response.json()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Model info error: {e}\")\n",
    "    \n",
    "    # Test 3: Sample prediction\n",
    "    try:\n",
    "        test_data = {\n",
    "            \"Gender\": \"Male\",\n",
    "            \"Married\": \"Yes\", \n",
    "            \"Dependents\": 1,\n",
    "            \"Education\": \"Graduate\",\n",
    "            \"Self_Employed\": \"No\",\n",
    "            \"ApplicantIncome\": 5000.0,\n",
    "            \"CoapplicantIncome\": 0.0,\n",
    "            \"LoanAmount\": 150.0,\n",
    "            \"Loan_Amount_Term\": 360.0,\n",
    "            \"Credit_History\": \"Yes\",\n",
    "            \"Property_Area\": \"Semiurban\"\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{base_url}/predict\",\n",
    "            json=test_data,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Prediction test: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"   Prediction: {result['loan_status_text']}\")\n",
    "            print(f\"   Probability: {result['probability']:.4f}\")\n",
    "        else:\n",
    "            print(f\"   Error: {response.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Prediction test error: {e}\")\n",
    "        \n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "test_backend_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE VERIFICATION - Check everything is ready for deployment\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîç COMPLETE DEPLOYMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Check if we're in the right directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"üìÇ Current directory: {current_dir}\")\n",
    "\n",
    "if not current_dir.endswith('server'):\n",
    "    print(\"‚ö†Ô∏è Not in server directory. Changing...\")\n",
    "    if os.path.exists('server'):\n",
    "        os.chdir('server')\n",
    "        print(f\"‚úÖ Changed to: {os.getcwd()}\")\n",
    "\n",
    "# 2. Check for model file\n",
    "model_file = 'loan_model.pkl'\n",
    "if os.path.exists(model_file):\n",
    "    file_size = os.path.getsize(model_file)\n",
    "    print(f\"‚úÖ Model file found: {model_file} ({file_size} bytes)\")\n",
    "    \n",
    "    # 3. Test loading the model\n",
    "    try:\n",
    "        model_data = joblib.load(model_file)\n",
    "        \n",
    "        if hasattr(model_data, 'predict'):\n",
    "            model = model_data\n",
    "            print(\"‚úÖ Model format: Standard sklearn model\")\n",
    "        elif isinstance(model_data, dict) and 'model' in model_data:\n",
    "            model = model_data['model']\n",
    "            print(\"‚úÖ Model format: Dictionary with metadata\")\n",
    "            print(f\"   Features: {model_data.get('feature_names', 'Not specified')}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Unknown model format: {type(model_data)}\")\n",
    "            model = None\n",
    "        \n",
    "        if model:\n",
    "            # 4. Test prediction\n",
    "            test_input = np.array([[1, 1, 1, 1, 0, 5000, 1000, 150, 360, 1, 1]])\n",
    "            prediction = model.predict(test_input)[0]\n",
    "            print(f\"‚úÖ Test prediction: {prediction} ({'Approved' if prediction == 1 else 'Not Approved'})\")\n",
    "            \n",
    "            # 5. Test probability\n",
    "            try:\n",
    "                probability = model.predict_proba(test_input)[0]\n",
    "                print(f\"‚úÖ Test probability: {probability}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Probability test failed: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model loading failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"‚ùå Model file NOT found: {model_file}\")\n",
    "    print(\"Run the training cell (Cell 3) to create the model\")\n",
    "\n",
    "# 6. Check required files\n",
    "required_files = ['api.py', 'config.py', '.env', 'requirements.txt']\n",
    "print(f\"\\nüìã Checking required files:\")\n",
    "for file in required_files:\n",
    "    exists = \"‚úÖ\" if os.path.exists(file) else \"‚ùå\"\n",
    "    print(f\"   {exists} {file}\")\n",
    "\n",
    "# 7. Check CSV data file\n",
    "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "print(f\"\\nüìä CSV files: {csv_files}\")\n",
    "\n",
    "# 8. Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if os.path.exists(model_file) and model is not None:\n",
    "    print(\"üéâ READY FOR DEPLOYMENT!\")\n",
    "    print(\"‚úÖ Model file exists and loads correctly\")\n",
    "    print(\"‚úÖ Predictions working\")\n",
    "    print(\"\\nüöÄ Next steps:\")\n",
    "    print(\"1. Open terminal in server directory\")\n",
    "    print(\"2. Run: python api.py\")\n",
    "    print(\"3. Open browser to: http://localhost:8000/docs\")\n",
    "else:\n",
    "    print(\"‚ùå NOT READY - Issues detected\")\n",
    "    print(\"üí° Fix:\")\n",
    "    print(\"1. Run Cell 3 (training pipeline)\")\n",
    "    print(\"2. Verify loan_model.pkl is created\")\n",
    "    print(\"3. Run this cell again to verify\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMERGENCY FIX: Verify and create model file in correct location\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üö® EMERGENCY MODEL FILE CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get the absolute path to server directory\n",
    "server_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "if not str(server_dir).endswith('server'):\n",
    "    if (server_dir / 'server').exists():\n",
    "        server_dir = server_dir / 'server'\n",
    "    elif (server_dir.parent / 'server').exists():\n",
    "        server_dir = server_dir.parent / 'server'\n",
    "\n",
    "print(f\"üìÇ Server directory: {server_dir}\")\n",
    "print(f\"üìÇ Current working directory: {Path.cwd()}\")\n",
    "\n",
    "# Change to server directory\n",
    "try:\n",
    "    os.chdir(server_dir)\n",
    "    print(f\"‚úÖ Changed to: {os.getcwd()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not change directory: {e}\")\n",
    "\n",
    "# List all files\n",
    "print(f\"\\nüìÅ Files in server directory:\")\n",
    "all_files = list(Path('.').glob('*'))\n",
    "for f in all_files[:20]:\n",
    "    if f.is_file():\n",
    "        print(f\"   {f.name} ({f.stat().st_size} bytes)\")\n",
    "\n",
    "# Check specifically for pkl files\n",
    "pkl_files = list(Path('.').glob('*.pkl'))\n",
    "print(f\"\\nüì¶ PKL files found: {len(pkl_files)}\")\n",
    "for pkl in pkl_files:\n",
    "    print(f\"   ‚úÖ {pkl.name} ({pkl.stat().st_size} bytes)\")\n",
    "\n",
    "# Check if loan_model.pkl exists\n",
    "model_file = Path('loan_model.pkl')\n",
    "if model_file.exists():\n",
    "    print(f\"\\n‚úÖ loan_model.pkl EXISTS!\")\n",
    "    print(f\"   üìç Location: {model_file.absolute()}\")\n",
    "    print(f\"   üìè Size: {model_file.stat().st_size} bytes\")\n",
    "    \n",
    "    # Try to load it\n",
    "    try:\n",
    "        import joblib\n",
    "        model_data = joblib.load(model_file)\n",
    "        print(f\"   ‚úÖ Can be loaded successfully\")\n",
    "        print(f\"   üìä Type: {type(model_data)}\")\n",
    "        \n",
    "        if hasattr(model_data, 'predict'):\n",
    "            print(f\"   ‚úÖ Has predict method - ready for API\")\n",
    "        elif isinstance(model_data, dict) and 'model' in model_data:\n",
    "            print(f\"   ‚úÖ Dictionary format - has model key\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Cannot load: {e}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå loan_model.pkl NOT FOUND!\")\n",
    "    print(f\"   Expected at: {model_file.absolute()}\")\n",
    "    print(f\"\\nüí° SOLUTION: Run Cell 3 (training pipeline) to create the model\")\n",
    "    print(f\"   Or if model exists elsewhere, copy it to: {server_dir}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
